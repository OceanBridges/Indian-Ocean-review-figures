---
title: "DADA2 amplicon analysis for Io Review"
author: "subhadeep CHOWDHURY"
date: "9th December 2022"
output: html_document
---

```{r setup, include=FALSE}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("dada2")


library("dada2")

```

## Load files and processing
In order to load our files, we make a path to our files and list them to make sure everything it there. secondly we need to math R1 with R2 by doing some strin manipulation

```{r files load}

path <- "C:/Users/subhadeep/Desktop/Review/Sequence data/area wize analysis/BoB"

list.files(path)

# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq

fnFs <- sort(list.files(path, pattern="_1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq

sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

```


```{r}
#Sanity check - does reads overlap? 

fnF <- "C:/Users/subhadeep/Desktop/Review/Sequence data/area wize analysis/BoB/DRR393951_1.fastq"

fnR <- "C:/Users/subhadeep/Desktop/Review/Sequence data/area wize analysis/BoB/DRR393951_2.fastq"

sqF <- getSequences(derepFastq(fnF))

sqR <- getSequences(derepFastq(fnR))
unname(outer(sqF[1:5], rc(sqR[1:5]), nwhamming))
nwalign(sqF[2], rc(sqR[4]))

#Based on nwalign, we can see an overlap of 90 bases between rwd and rev reads. 
```


## Inspect read quality profiles

We will now inspect the quality of fwd reads and revers reads
In gray-scale is a heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same length, hence the flat red line).

You can manipulate which plot you want to see by using "[x:x]" e.g. "[1]" shows the first file, [1:2]" shows the first two files and "[3:4]" shows files 3 and 4 e.t.c.

```{r Quality, echo=FALSE}


#Quality plot of forward reads

plotQualityProfile(fnFs[1:2])

```

```{r Quality, echo=FALSE}

#Quality plot of revers reads

plotQualityProfile(fnRs[1:2])

```
Above plots shows that revers reads have a significant worse quality. This is common and ok. Dada2 have a error model where the quality information is included.


Next, we filter the reads and trim

```{r Quality, echo=FALSE}

# Place filtered files in filtered/ subdirectory

filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq"))

names(filtFs) <- sample.names
names(filtRs) <- sample.names

#trimming and filtering. We’ll use standard filtering parameters: maxN=0 (DADA2 requires no Ns), truncQ=2, rm.phix=TRUE and maxEE=2. The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.





out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft = 17, truncLen=c(200,180),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=FALSE)

#summary, to see how many reads were removed by filtereing

head(out)

```

##Learning the error rate
The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The  learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r Errorrate of fwd and rev reads, echo=FALSE}

errF <- learnErrors(filtFs, multithread=TRUE)

errR <- learnErrors(filtRs, multithread=TRUE)

#As a sanity check, we plot the errors (errF for fwd reads and errR for revers)

plotErrors(errF, nominalQ=TRUE)

```
The error rates for each possible transition (A -> C, A -> G, ....) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence.


####Dereplication
Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance” equal to the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons.

Dereplication in the DADA2 pipeline has one crucial addition from other pipelines: DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent sample inference step, significantly increasing DADA2’s accuracy.



derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names







 
##Data inference algorithm (see paper). Basiccaly, we identify how many unique samples we have in each sample.

```{r Data inference, echo=FALSE}

dadaFs <- dada(derepFs,err=errF, multithread=TRUE)

dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

dadaFs[1]

dadaRs[1]
```

##merged Paried reads
We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged "contig" sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r merge reads, echo=FALSE}

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

# Inspect the merger data.frame from the first sample

head(mergers[[1]])

```

##Construct sequences table. We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r sequences table, echo=FALSE}

seqtab <- makeSequenceTable(mergers)

dim(seqtab)

table(nchar(getSequences(seqtab)))

#filtering out for amplicon lenght
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 300:334] 

dim(seqtab2)

table(nchar(getSequences(seqtab2)))



```

##Remove chimera
The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant "parent" sequences.

```{r remoce chimera, echo=FALSE}

seqtab.nochim <- removeBimeraDenovo(seqtab2, minFoldParentOverAbundance=8, method="consensus", multithread=TRUE, verbose=TRUE)


seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)

dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab2)

```

##Track reads through pipeline

```{r amount of reads after pipeline, echo=FALSE}

getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)

#From above codes, we can now track how manyu reads were lost through the pipeline. 
#Based on below table, I would say it is pretty good. 

```

```{r}

#Now, we will extract extract sequences to work with downstreams

asv_seqs <- colnames(seqtab.nochim)

asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

## making and writing out a fasta of our final ASV seqs:

asv_fasta <- c(rbind(asv_headers, asv_seqs))

head(asv_fasta)

write(asv_fasta, "ASVs_BoB_whole area.fasta")

## count table:

asv_tab <- t(seqtab.nochim)


row.names(asv_tab) <- sub(">", "", asv_headers)

write.table(asv_tab, "ASVs_BoB_whole area.tsv", sep="\t", quote=F, col.names=NA)

```

##Now the fun part - taxonomic assignment

You can also do species assignment - taxonomic assignment only goes down to family. 
However species assignemnt can mess the taxonomic assignment up. 

```{r taxonomic, echo=FALSE}

####nifH database assign taxonomy
download.file(url="https://raw.githubusercontent.com/moyn413/nifHdada2/master/nifH_dada2_phylum_v1.1.0.fasta", destfile="nifH_dada2_phyllum_v1.1.0.fasta")
taxa <- assignTaxonomy(seqtab.nochim, "nifH_dada2_phyllum_v1.1.0.fasta" , multithread=TRUE, minBoot = 0)





#Lets look at the taxonomi

taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)


#write table
write.table(taxa.print, "ASVs_Taxonomy_BoB_whole area.tsv", sep = "\t", quote=F, col.names=NA)

```

